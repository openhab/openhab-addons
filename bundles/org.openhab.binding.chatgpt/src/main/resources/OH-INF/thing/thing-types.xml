<?xml version="1.0" encoding="UTF-8"?>
<thing:thing-descriptions bindingId="chatgpt"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xmlns:thing="https://openhab.org/schemas/thing-description/v1.0.0"
	xsi:schemaLocation="https://openhab.org/schemas/thing-description/v1.0.0 https://openhab.org/schemas/thing-description-1.0.0.xsd">

	<thing-type id="account" extensible="chat">

		<label>OpenAI Account</label>
		<description>Account at OpenAI that is used for accessing the ChatGPT API.</description>

		<channels>
			<channel id="chat" typeId="chat"/>
		</channels>

		<config-description>
			<parameter-group name="authentication">
				<label>Authentication</label>
				<description>Authentication for connecting to OpenAI API.</description>
			</parameter-group>
			<parameter-group name="hli">
				<label>HLI Configuration</label>
				<description>Configure HLI service.</description>
			</parameter-group>
			<parameter name="apiKey" type="text" required="true" groupName="authentication">
				<context>password</context>
				<label>API Key</label>
				<description>API key to access the account</description>
			</parameter>
			<parameter name="apiUrl" type="text" required="false" groupName="authentication">
				<label>API URL</label>
				<description>The server API where to reach the AI service.</description>
				<default>https://api.openai.com/v1/chat/completions</default>
				<advanced>true</advanced>
				<options>
					<option value="https://api.openai.com/v1/chat/completions">https://api.openai.com/v1/chat/completions</option>
				</options>
				<limitToOptions>false</limitToOptions>
			</parameter>
			<parameter name="modelUrl" type="text" required="false" groupName="authentication">
				<label>Model URL</label>
				<description>The model url where to retrieve the available models from.</description>
				<default>https://api.openai.com/v1/models</default>
				<advanced>true</advanced>
				<options>
					<option value="https://api.openai.com/v1/models">https://api.openai.com/v1/models</option>
				</options>
				<limitToOptions>false</limitToOptions>
			</parameter>
			<parameter name="model" type="text" required="true" groupName="hli">
				<label>Model</label>
				<description>ID of the model to use.</description>
				<default>gpt-4o-mini</default>
				<advanced>true</advanced>
			</parameter>
			<parameter name="temperature" type="decimal" min="0" max="2" groupName="hli">
				<label>Temperature</label>
				<description>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more
					random, while lower values like 0.2 will make it more focused and deterministic.</description>
				<default>0.5</default>
				<advanced>true</advanced>
			</parameter>
			<parameter name="topP" type="decimal" min="0" max="1" groupName="hli">
				<label>TopP</label>
				<description>
					An alternative to sampling with temperature, called nucleus sampling, where the model considers the
					results of the
					tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability
					mass are considered.
					We generally recommend altering this or temperature but not both.
				</description>
				<default>1</default>
				<advanced>true</advanced>
			</parameter>
			<parameter name="systemMessage" type="text" groupName="hli">
				<label>System Message</label>
				<description>Override the default system message of the assistant.</description>
				<default>You are the manager of the openHAB smart home. You know how to manage devices in a smart home or provide
					their current status. You can also answer a question not related to devices in the house. Or, for example, you can
					compose a story upon request.
					I will provide information about the smart home; if necessary, you can perform the
					function; if there is not enough
					information to perform it, then clarify briefly, without listing all the available
					devices and parameters for the
					function. If the question is not related to devices in a smart home, then answer the
					question briefly,
					maximum 3 sentences in everyday language.

					The name, current status and location of devices is
					displayed in 'Available devices'.
					Use the items_control function only for the requested action, not for providing
					current states.

					Available devices:
				</default>
				<advanced>true</advanced>
			</parameter>
			<parameter name="maxTokens" type="integer" groupName="hli">
				<label>Max Tokens</label>
				<description>The maximum number of tokens that can be generated in the chat completion.</description>
				<default>1000</default>
				<advanced>true</advanced>
			</parameter>
			<parameter name="keepContext" type="integer" groupName="hli">
				<label>Keep Context</label>
				<description>How long to store the context in minutes.</description>
				<default>2</default>
				<advanced>true</advanced>
			</parameter>
			<parameter name="contextThreshold" type="integer" groupName="hli">
				<label>Context Threshold</label>
				<description>Limit total tokens included in context.</description>
				<default>10000</default>
				<advanced>true</advanced>
			</parameter>
			<parameter name="useSemanticModel" type="boolean" groupName="hli">
				<label>Use Semantic Model</label>
				<description>Use a semantic model to determine the location of an item. Otherwise, item names must begin with a
					"location_" e.g. "Kitchen_Light"</description>
				<default>true</default>
				<advanced>true</advanced>
			</parameter>
		</config-description>
	</thing-type>

	<channel-type id="chat">
		<item-type>String</item-type>
		<label>Chat</label>
		<description>A chat session</description>
		<autoUpdatePolicy>veto</autoUpdatePolicy>
		<config-description>
			<parameter name="model" type="text">
				<label>Model</label>
				<description>The model to be used for the responses</description>
				<limitToOptions>false</limitToOptions>
				<default>gpt-4o-mini</default>
			</parameter>
			<parameter name="temperature" type="decimal" min="0" max="2">
				<label>Temperature</label>
				<description>Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more
					focused and deterministic.</description>
				<default>0.5</default>
			</parameter>
			<parameter name="systemMessage" type="text">
				<label>System Message</label>
				<description>The system message helps set the behavior of the assistant.</description>
			</parameter>
			<parameter name="maxTokens" type="decimal">
				<label>Maximum Number of Tokens</label>
				<description>The maximum number of tokens to generate in the completion.</description>
				<default>1000</default>
				<advanced>true</advanced>
			</parameter>
			<parameter name="topP" type="decimal" min="0" max="1">
				<label>TopP</label>
				<description>An alternative to sampling with temperature, called nucleus sampling, where the model considers the
					results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability
					mass are considered. We generally recommend altering this or temperature but not both.</description>
				<default>1</default>
				<advanced>true</advanced>
			</parameter>
		</config-description>
	</channel-type>
</thing:thing-descriptions>
